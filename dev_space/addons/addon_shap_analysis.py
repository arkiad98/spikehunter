# addons/addon_shap_analysis.py (ver 2.7.12)
"""
SHAP(SHapley Additive exPlanations) ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬
ML ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ í•´ì„í•˜ê³  ì„¤ëª…í•˜ëŠ” ë¶„ì„ ì• ë“œì˜¨ ëª¨ë“ˆ.

[v2.7.12] íšŒê·€ ëª¨ë¸ ë¶„ì„ ë° íƒ€ì„ìŠ¤íƒ¬í”„ íŒŒì¼ëª… ì ìš©
- íšŒê·€ ëª¨ë¸(target_model.joblib)ì— ëŒ€í•œ SHAP ë¶„ì„ ê¸°ëŠ¥ ì¶”ê°€.
- ëª¨ë“  SHAP ë¦¬í¬íŠ¸ ì´ë¯¸ì§€ íŒŒì¼ëª…ì— íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ì¶”ê°€í•˜ì—¬ ë¶„ì„ ì´ë ¥ ê´€ë¦¬.
"""
import os
import pandas as pd
import numpy as np
import lightgbm as lgb
import joblib
import shap
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
from datetime import datetime
import optuna

from modules.utils_io import read_yaml, ensure_dir, get_stock_names, load_partition_day
from modules.utils_logger import logger
from modules import utils_db

def _set_korean_font():
    """matplotlibì—ì„œ í•œê¸€ í°íŠ¸ë¥¼ ì„¤ì •í•˜ì—¬ ê·¸ë˜í”„ì˜ ê°€ë…ì„±ì„ ë†’ì…ë‹ˆë‹¤."""
    font_path = 'c:/Windows/Fonts/malgun.ttf' # Windows ê¸°ì¤€
    if not os.path.exists(font_path):
        font_path = '/usr/share/fonts/truetype/nanum/NanumGothic.ttf'
        
    if os.path.exists(font_path):
        font_name = fm.FontProperties(fname=font_path).get_name()
        plt.rc('font', family=font_name)
    else:
        logger.warning("í•œê¸€ í°íŠ¸(ë§‘ì€ ê³ ë”• ë˜ëŠ” ë‚˜ëˆ”ê³ ë”•)ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ê¸°ë³¸ í°íŠ¸ë¡œ ì¶œë ¥ë©ë‹ˆë‹¤.")
    plt.rcParams['axes.unicode_minus'] = False

def _get_latest_prediction_file(pred_path: str):
    """predictions í´ë”ì—ì„œ ê°€ì¥ ìµœì‹  ì¶”ì²œ ì¢…ëª© CSV íŒŒì¼ì„ ì°¾ìŠµë‹ˆë‹¤."""
    if not os.path.exists(pred_path): return None
    files = [os.path.join(pred_path, f) for f in os.listdir(pred_path) if f.endswith('.csv') and '_targets' not in f]
    if not files: return None
    return max(files, key=os.path.getctime)

def run_optimization_shap_analysis(study: optuna.study.Study, output_dir: str):
    """
    ì™„ë£Œëœ Optuna Study ê°ì²´ë¥¼ ë¶„ì„í•˜ì—¬ í•˜ì´í¼íŒŒë¼ë¯¸í„°ì˜ ì¤‘ìš”ë„ë¥¼ ì‹œê°í™”í•˜ê³  DBì— ê¸°ë¡í•©ë‹ˆë‹¤.
    """
    logger.info("ìµœì í™” ê²°ê³¼ì— ëŒ€í•œ SHAP ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    df = study.trials_dataframe()
    df = df[df['state'] == 'COMPLETE']
    
    if len(df) < 10:
        logger.warning(f"ë¶„ì„í•  Trialì˜ ìˆ˜ê°€ ë„ˆë¬´ ì ì–´({len(df)}ê°œ) SHAP ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return

    param_cols = [col for col in df.columns if col.startswith('params_')]
    X = df[param_cols]
    X.columns = [col.replace('params_', '') for col in X.columns]
    y = df['value']

    surrogate_model = lgb.LGBMRegressor(random_state=42, n_jobs=1)
    surrogate_model.fit(X, y)

    explainer = shap.TreeExplainer(surrogate_model)
    shap_values = explainer.shap_values(X)

    shap_df = pd.DataFrame({
        'item_name': X.columns,
        'mean_abs_shap': np.abs(shap_values).mean(axis=0)
    })
    shap_df = shap_df.sort_values('mean_abs_shap', ascending=False)
    shap_df['rank'] = range(1, len(shap_df) + 1)
    
    utils_db.insert_shap_results(
        run_id=study.study_name,
        analysis_type='optimization_hyperparameter',
        shap_df=shap_df
    )
    
    _set_korean_font()
    plt.figure()
    shap.summary_plot(shap_values, X, plot_type="bar", show=False)
    # [ìˆ˜ì •] Study ì´ë¦„(ìµœì í™” ë‚´ìš© + íƒ€ì„ìŠ¤íƒ¬í”„)ì„ ì œëª©ì— ë™ì ìœ¼ë¡œ ì¶”ê°€
    title = f"Hyperparameter Importance (SHAP)\nStudy: {study.study_name}"
    plt.title(title, fontsize=12)
    plt.xlabel("í‰ê·  SHAP ê°’ (ì„±ê³¼ì— ëŒ€í•œ ê¸°ì—¬ë„)", fontsize=12)
    plt.tight_layout()

    output_path = os.path.join(output_dir, "optimization_shap_summary.png")
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    plt.close()
    
    logger.info(f"ìµœì í™” íŒŒë¼ë¯¸í„° SHAP ë¶„ì„ ê²°ê³¼ê°€ '{output_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


def run_shap_analysis(settings_path: str, analysis_type: str = 'global_classification'):
    """
    SHAP ë¶„ì„ì„ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ì‹œê°í™” ë° DBì— ì €ì¥í•©ë‹ˆë‹¤.
    """
    run_timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    logger.info("="*60)
    logger.info(f"      <<< SHAP ëª¨ë¸ í•´ì„ ({analysis_type}) ì‹œì‘ >>>")
    logger.info("="*60)
    
    cfg = read_yaml(settings_path)
    paths = cfg["paths"]

    # 1. ë¶„ì„ ìœ í˜•ì— ë”°ë¼ ëª¨ë¸, ë°ì´í„°ì…‹, íƒ€ê²Ÿ ì»¬ëŸ¼ ì„¤ì •
    if analysis_type == 'global_classification':
        model_filename = "lgbm_model.joblib"
        dataset_filename = "ml_classification_dataset.parquet"
        target_col = 'target'
        title = "ë¶„ë¥˜ ëª¨ë¸ SHAP ì „ì—­ í”¼ì²˜ ì¤‘ìš”ë„"
    elif analysis_type == 'global_regression':
        model_filename = "target_model.joblib"
        dataset_filename = "ml_regression_dataset.parquet"
        target_col = 'target_max_ret'
        title = "íšŒê·€ ëª¨ë¸ SHAP ì „ì—­ í”¼ì²˜ ì¤‘ìš”ë„"
    elif analysis_type == 'local':
        model_filename = "lgbm_model.joblib" # Local ë¶„ì„ì€ ë¶„ë¥˜ ëª¨ë¸ ê¸°ì¤€
    else:
        logger.error(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë¶„ì„ ìœ í˜•ì…ë‹ˆë‹¤: {analysis_type}")
        return

    # 2. ëª¨ë¸ ë¡œë“œ
    model_path = os.path.join(paths["models"], model_filename)
    if not os.path.exists(model_path):
        logger.error(f"í•™ìŠµëœ ML ëª¨ë¸({model_path})ì´ ì—†ìŠµë‹ˆë‹¤. ëª¨ë¸ í•™ìŠµì„ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        return
    model = joblib.load(model_path)
    logger.info(f"ML ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_path}")

    # 3. SHAP Explainer ìƒì„±
    logger.info("SHAP TreeExplainerë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")
    explainer = shap.TreeExplainer(model)

    # 4. ì „ì—­ ë¶„ì„ (Global Analysis)
    if analysis_type.startswith('global'):
        dataset_path = os.path.join(paths["ml_dataset"], dataset_filename)
        if not os.path.exists(dataset_path):
            logger.error(f"ML ë°ì´í„°ì…‹({dataset_path})ì´ ì—†ìŠµë‹ˆë‹¤. ë°ì´í„°ì…‹ ìƒì„±ì„ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
            return
        df = pd.read_parquet(dataset_path)
        feature_cols = [col for col in df.columns if col not in ['date', 'code', target_col]]
        X, y = df[feature_cols], df[target_col]
        from sklearn.model_selection import train_test_split
        _, X_test, _, _ = train_test_split(X, y, test_size=0.2, shuffle=False)

        logger.info("ì „ì²´ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì— ëŒ€í•œ SHAP ê°’ì„ ê³„ì‚°í•©ë‹ˆë‹¤...")
        shap_values = explainer.shap_values(X_test)
        logger.info("SHAP ê°’ ê³„ì‚° ì™„ë£Œ.")
        
        shap_values_for_calc = shap_values[1] if isinstance(shap_values, list) else shap_values
        
        shap_df = pd.DataFrame({
            'item_name': X_test.columns,
            'mean_abs_shap': np.abs(shap_values_for_calc).mean(axis=0)
        })
        shap_df = shap_df.sort_values('mean_abs_shap', ascending=False)
        shap_df['rank'] = range(1, len(shap_df) + 1)

        utils_db.insert_shap_results(
            run_id=f"model_train_{run_timestamp}",
            analysis_type=analysis_type,
            shap_df=shap_df
        )

        _set_korean_font()
        plt.figure()
        shap.summary_plot(shap_values, X_test, plot_type="bar", show=False)
        plt.title(title, fontsize=16)
        plt.xlabel("í‰ê·  SHAP ê°’ (ì˜ˆì¸¡ì— ëŒ€í•œ ê¸°ì—¬ë„)", fontsize=12)
        plt.tight_layout()

        # ğŸ”´ [ìˆ˜ì •] íŒŒì¼ëª…ì— íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€
        output_filename = f"shap_summary_{analysis_type}_{run_timestamp}.png"
        output_path = os.path.join(paths["models"], output_filename)
        plt.savefig(output_path)
        plt.close()
        logger.info(f"SHAP Summary Plotì´ '{output_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
    # 5. ê°œë³„ ë¶„ì„ (Local Analysis)
    elif analysis_type == 'local':
        latest_pred_file = _get_latest_prediction_file(paths["predictions"])
        if not latest_pred_file:
            logger.error("ë¶„ì„í•  ì¶”ì²œ ì¢…ëª© íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. '1. ì˜¤ëŠ˜ì˜ ì¶”ì²œ ì¢…ëª© ìƒì„±'ì„ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
            return
        
        logger.info(f"ìµœì‹  ì¶”ì²œ ì¢…ëª© íŒŒì¼ ë¡œë“œ: {latest_pred_file}")
        recs_df = pd.read_csv(latest_pred_file, dtype={'code': str})
        pred_date_str = os.path.basename(latest_pred_file).replace('.csv', '')
        pred_date = pd.to_datetime(pred_date_str)

        features_today = load_partition_day(paths["features"], pred_date, pred_date)
        if features_today.empty:
            logger.error(f"{pred_date.date()} ë‚ ì§œì˜ í”¼ì²˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        recs_with_features = pd.merge(recs_df, features_today, on='code', how='left')
        
        report_dir = os.path.join(paths["backtest"], "shap_reports", pred_date_str)
        ensure_dir(report_dir)
        logger.info(f"ê°œë³„ ë¶„ì„ ë¦¬í¬íŠ¸ëŠ” ë‹¤ìŒ í´ë”ì— ì €ì¥ë©ë‹ˆë‹¤: {report_dir}")

        for _, row in recs_with_features.iterrows():
            X_rec = row[model.feature_name_].to_frame().T
            X_rec = X_rec.apply(pd.to_numeric, errors='coerce').fillna(0)
            
            shap_values_local = explainer.shap_values(X_rec)
            
            if isinstance(explainer.expected_value, (list, np.ndarray)):
                expected_value = explainer.expected_value[1]
                shap_values_for_class1 = shap_values_local[1]
            else:
                expected_value = explainer.expected_value
                shap_values_for_class1 = shap_values_local

            plot_title = f"ì¢…ëª© {row['code']} ({row['name']}) SHAP ë¶„ì„ (ì˜ˆì¸¡ ì ìˆ˜: {row['score']:.2f})"
            _set_korean_font()
            
            shap.force_plot(expected_value, shap_values_for_class1, X_rec, matplotlib=True, show=False, text_rotation=15)
            plt.title(plot_title, fontsize=12)
            plt.tight_layout()
            
            filename = f"{int(row['rank']):02d}_{row['code']}_{row['name']}_shap.png"
            output_path = os.path.join(report_dir, filename)
            plt.savefig(output_path, dpi=150, bbox_inches='tight')
            plt.close()
        
        logger.info(f"ì´ {len(recs_with_features)}ê°œ ì¢…ëª©ì— ëŒ€í•œ SHAP Force Plot ìƒì„±ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.")

    logger.info("="*60)

