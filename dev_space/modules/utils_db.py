# modules/utils_db.py (ver 2.7.10)
"""DB ê´€ë¦¬ ìœ í‹¸ë¦¬í‹° ëª¨ë“ˆ.

[v2.7.10] SHAP ë¶„ì„ ê²°ê³¼ ë¡œê¹… ê¸°ëŠ¥ ì¶”ê°€
- SHAP ë¶„ì„ ê²°ê³¼ë¥¼ ì €ì¥í•˜ê¸° ìœ„í•œ 'shap_importance_log' í…Œì´ë¸” ì •ì˜ ì¶”ê°€.
- SHAP ë¶„ì„ ê²°ê³¼ë¥¼ DBì— ì‚½ì…í•˜ëŠ” 'insert_shap_results' í•¨ìˆ˜ ì‹ ì„¤.
"""
import sqlite3
import pandas as pd
import json
import numpy as np
from datetime import datetime
import hashlib
import time
import os
import typing

if typing.TYPE_CHECKING:
    import optuna

from .utils_logger import logger

DB_PATH = "data/db/spikehunter_log.db"
TABLE_DEFINITIONS = {
    "backtest_summary": """
        CREATE TABLE IF NOT EXISTS backtest_summary (
            backtest_id TEXT PRIMARY KEY,
            run_timestamp TEXT NOT NULL,
            strategy_name TEXT,
            start_date TEXT,
            end_date TEXT,
            cagr REAL,
            sharpe REAL,
            mdd REAL,
            total_trades INTEGER,
            win_rate REAL,
            params_json TEXT,
            equity_file_path TEXT
        );
    """,
    "trade_log": """
        CREATE TABLE IF NOT EXISTS trade_log (
            trade_id INTEGER PRIMARY KEY AUTOINCREMENT,
            backtest_id TEXT,
            entry_date TEXT,
            exit_date TEXT,
            code TEXT,
            return REAL,
            reason TEXT,
            FOREIGN KEY (backtest_id) REFERENCES backtest_summary (backtest_id)
        );
    """,
    "optimization_log": """
        CREATE TABLE IF NOT EXISTS optimization_log (
            log_id INTEGER PRIMARY KEY AUTOINCREMENT,
            study_name TEXT NOT NULL,
            trial_number INTEGER NOT NULL,
            state TEXT,
            value REAL,
            params_json TEXT,
            run_timestamp TEXT
        );
    """,
    # ğŸ”´ [ì¶”ê°€] SHAP ë¶„ì„ ê²°ê³¼ ì €ì¥ì„ ìœ„í•œ ì‹ ê·œ í…Œì´ë¸”
    "shap_importance_log": """
        CREATE TABLE IF NOT EXISTS shap_importance_log (
            log_id INTEGER PRIMARY KEY AUTOINCREMENT,
            run_id TEXT NOT NULL,
            analysis_type TEXT NOT NULL,
            item_name TEXT NOT NULL,
            mean_abs_shap REAL,
            rank INTEGER,
            run_timestamp TEXT
        );
    """
}

# [ìˆ˜ì •] _sanitize_for_db í•¨ìˆ˜ë¥¼ ë” ì•ˆì •ì ì¸ ë²„ì „ìœ¼ë¡œ êµì²´í•©ë‹ˆë‹¤.
def _sanitize_for_db(value):
    """NumPy/Pandas íƒ€ì…ì„ DBì— ì €ì¥ ê°€ëŠ¥í•œ íŒŒì´ì¬ ê¸°ë³¸ íƒ€ì…ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    # pd.isnaëŠ” None, np.nan ë“±ì„ ëª¨ë‘ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    if pd.isna(value):
        return None
    # numpy float ë˜ëŠ” python floatë¥¼ python floatìœ¼ë¡œ ëª…ì‹œì  ë³€í™˜
    if isinstance(value, (np.floating, float)):
        return float(value)
    # numpy int ë˜ëŠ” python intë¥¼ python intë¡œ ëª…ì‹œì  ë³€í™˜
    if isinstance(value, (np.integer, int)):
        return int(value)
    if isinstance(value, np.ndarray):
        return value.tolist()
    if isinstance(value, pd.Timestamp):
        return value.isoformat()
    return value # ê·¸ ì™¸ íƒ€ì…(ì£¼ë¡œ str)ì€ ê·¸ëŒ€ë¡œ ë°˜í™˜

def get_db_connection():
    """DB ì—°ê²°ì„ ìƒì„±í•˜ê³  ë°˜í™˜í•©ë‹ˆë‹¤."""
    os.makedirs(os.path.dirname(DB_PATH), exist_ok=True)
    return sqlite3.connect(DB_PATH, timeout=30)

def create_tables():
    """í”„ë¡œê·¸ë¨ ì‹œì‘ ì‹œ í•„ìš”í•œ ëª¨ë“  í…Œì´ë¸”ì„ ìƒì„±í•˜ê±°ë‚˜ ê²€ì¦í•©ë‹ˆë‹¤."""
    try:
        with get_db_connection() as conn:
            cursor = conn.cursor()
            for table_name, ddl_script in TABLE_DEFINITIONS.items():
                cursor.execute(ddl_script)
            conn.commit()
        logger.info(f"ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸”ì´ ì„±ê³µì ìœ¼ë¡œ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤. (ê²½ë¡œ: {DB_PATH})")
    except Exception as e:
        logger.error(f"DB í…Œì´ë¸” ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)

def insert_backtest_results(backtest_id: str, metrics: dict, params: dict, tradelog_df: pd.DataFrame, equity_path: str, start_date: str, end_date: str, strategy_name: str):
    """ë°±í…ŒìŠ¤íŠ¸ ìµœì¢… ê²°ê³¼ë¥¼ DBì— ì‚½ì…í•©ë‹ˆë‹¤."""
    # ... ê¸°ì¡´ ì½”ë“œ ...
    params_str = json.dumps({k: _sanitize_for_db(v) for k, v in params.items()}, ensure_ascii=False)

    if len(params_str) > 5000:
        logger.warning(f"DBì— ì €ì¥ë  íŒŒë¼ë¯¸í„° JSONì˜ ê¸¸ì´ê°€ 5000ìë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤ (ê¸¸ì´: {len(params_str)}).")

    summary_data = {
        'backtest_id': backtest_id,
        'run_timestamp': datetime.now().isoformat(),
        'strategy_name': strategy_name,
        'start_date': start_date,
        'end_date': end_date,
        'cagr': _sanitize_for_db(metrics.get('CAGR_raw', 0.0)),
        'sharpe': _sanitize_for_db(metrics.get('Sharpe_raw', 0.0)),
        'mdd': _sanitize_for_db(metrics.get('MDD_raw', 0.0)),
        'total_trades': _sanitize_for_db(metrics.get('ì´ê±°ë˜íšŸìˆ˜', 0)),
        'win_rate': _sanitize_for_db(metrics.get('win_rate_raw', 0.0)),
        'params_json': params_str,
        'equity_file_path': equity_path
    }

    try:
        with get_db_connection() as conn:
            summary_df = pd.DataFrame([summary_data])
            summary_df.to_sql('backtest_summary', conn, if_exists='append', index=False)

            if not tradelog_df.empty:
                tradelog_to_insert = tradelog_df.copy()
                tradelog_to_insert['backtest_id'] = backtest_id

                tradelog_to_insert['entry_date'] = pd.to_datetime(tradelog_to_insert['entry_date']).map(lambda x: x.isoformat())
                tradelog_to_insert['exit_date'] = pd.to_datetime(tradelog_to_insert['exit_date']).map(lambda x: x.isoformat())

                tradelog_to_insert = tradelog_to_insert[['backtest_id', 'entry_date', 'exit_date', 'code', 'return', 'reason']]
                tradelog_to_insert.to_sql('trade_log', conn, if_exists='append', index=False)

            logger.info(f"ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ê°€ DBì— ì„±ê³µì ìœ¼ë¡œ ê¸°ë¡ë˜ì—ˆìŠµë‹ˆë‹¤ (ID: {backtest_id}).")
    except Exception as e:
        logger.error(f"DB ê¸°ë¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)

def insert_optimization_logs(study: 'optuna.study.Study'):
    """ì™„ë£Œëœ Optuna ìŠ¤í„°ë””ì˜ ëª¨ë“  Trial ê²°ê³¼ë¥¼ DBì— ì¼ê´„ ì‚½ì…í•©ë‹ˆë‹¤."""
    # ... ê¸°ì¡´ ì½”ë“œ ...
    if not study:
        return

    records = []
    run_ts = datetime.now().isoformat()
    for trial in study.trials:
        records.append({
            'study_name': study.study_name,
            'trial_number': trial.number,
            'state': trial.state.name,
            'value': trial.value,
            'params_json': json.dumps(trial.params),
            'run_timestamp': run_ts
        })

    if not records:
        logger.warning(f"'{study.study_name}' ìŠ¤í„°ë””ì—ì„œ DBì— ê¸°ë¡í•  Trialì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    try:
        with get_db_connection() as conn:
            df = pd.DataFrame(records)
            df.to_sql('optimization_log', conn, if_exists='append', index=False)
            logger.info(f"'{study.study_name}' ìŠ¤í„°ë””ì˜ Trial {len(records)}ê°œê°€ DBì— ì„±ê³µì ìœ¼ë¡œ ê¸°ë¡ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        logger.error(f"ìµœì í™” ë¡œê·¸ DB ê¸°ë¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)

# ğŸ”´ [ì¶”ê°€] SHAP ë¶„ì„ ê²°ê³¼ë¥¼ DBì— ì €ì¥í•˜ëŠ” í•¨ìˆ˜
def insert_shap_results(run_id: str, analysis_type: str, shap_df: pd.DataFrame):
    """
    SHAP ë¶„ì„ ê²°ê³¼(í”¼ì²˜ ì¤‘ìš”ë„ ë“±)ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ê¸°ë¡í•©ë‹ˆë‹¤.
    """
    if shap_df.empty:
        return
        
    records = shap_df.copy()
    records['run_id'] = run_id
    records['analysis_type'] = analysis_type
    records['run_timestamp'] = datetime.now().isoformat()
    
    # DB í…Œì´ë¸” ì»¬ëŸ¼ ìˆœì„œì— ë§ê²Œ ì¬ì •ë ¬
    records = records[['run_id', 'analysis_type', 'item_name', 'mean_abs_shap', 'rank', 'run_timestamp']]
    
    try:
        with get_db_connection() as conn:
            records.to_sql('shap_importance_log', conn, if_exists='append', index=False)
        logger.info(f"SHAP ë¶„ì„ ê²°ê³¼({analysis_type}, {len(records)}ê°œ)ê°€ DBì— ì„±ê³µì ìœ¼ë¡œ ê¸°ë¡ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        logger.error(f"SHAP ë¶„ì„ ê²°ê³¼ DB ê¸°ë¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
