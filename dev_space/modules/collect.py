# modules/collect.py (ver 2.8.17, API í˜¸ì¶œ ì œí•œ ë°©ì§€ ë¡œì§ ì ìš©)
"""ë°ì´í„° ìˆ˜ì§‘ ëª¨ë“ˆ.

[v2.8.17] ëŒ€í•œë¯¼êµ­ ê³µíœ´ì¼ ì²˜ë¦¬ ìµœì¢… ë¡œì§ ì ìš©
- ë¬¸ì œ: pd.date_range(freq="B")ê°€ ì£¼ë§ë§Œ ì œì™¸í•˜ê³  ê³µíœ´ì¼ì€ í¬í•¨í•˜ì—¬,
  ê³µíœ´ì¼ì— ë°ì´í„° ìˆ˜ì§‘ì„ ì‹œë„í•˜ë©° ë¶ˆí•„ìš”í•œ ì˜¤ë¥˜ ë¡œê·¸ê°€ ë°œìƒí•˜ëŠ” ë¬¸ì œ í™•ì¸.
- í•´ê²°: ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ ì‹œ, pykrxë¥¼ í†µí•´ ì „ì²´ ìˆ˜ì§‘ ê¸°ê°„ì˜ ì‹¤ì œ 'ì˜ì—…ì¼'
  ëª©ë¡ì„ ë¯¸ë¦¬ ì¡°íšŒí•˜ëŠ” ë¡œì§ì„ run_collect í•¨ìˆ˜ì— ì¶”ê°€. ì´ì œ ìˆ˜ì§‘ ë£¨í”„ëŠ”
  ì´ ì˜ì—…ì¼ ëª©ë¡ì„ ê¸°ì¤€ìœ¼ë¡œë§Œ ë™ì‘í•˜ì—¬ ê³µíœ´ì¼ì—ëŠ” API í˜¸ì¶œ ìì²´ë¥¼
  ì‹œë„í•˜ì§€ ì•Šë„ë¡ ê·¼ë³¸ì ìœ¼ë¡œ ìˆ˜ì •.
- íš¨ê³¼: ëª¨ë“  ì¢…ë¥˜ì˜ ë¹„ì˜ì—…ì¼(ì£¼ë§, ê³µíœ´ì¼, ëŒ€ì²´ê³µíœ´ì¼, ì„ì‹œê³µíœ´ì¼)ì„
  ì™„ë²½í•˜ê²Œ ê±´ë„ˆë›°ì–´ ë°ì´í„° ìˆ˜ì§‘ íš¨ìœ¨ì„±ì„ ê·¹ëŒ€í™”í•˜ê³ , ì˜¤ë¥˜ ë¡œê·¸ ë°œìƒì„
  ì›ì²œì ìœ¼ë¡œ ì°¨ë‹¨í•˜ì—¬ íŒŒì´í”„ë¼ì¸ì˜ ì•ˆì •ì„±ì„ ì™„ì„±.

[ìˆ˜ì •] API í˜¸ì¶œ ì œí•œ ë°©ì§€ë¥¼ ìœ„í•´ time.sleep() ì¶”ê°€
- ë¬¸ì œ: ì§§ì€ ì‹œê°„ ì•ˆì— ê³¼ë„í•œ API ìš”ì²­ìœ¼ë¡œ IPê°€ ì°¨ë‹¨ë˜ê±°ë‚˜ ìš”ì²­ì´ ê±°ë¶€ë  ìˆ˜ ìˆëŠ” ë¬¸ì œ.
- í•´ê²°: ì¼ë³„ ë°ì´í„° ìˆ˜ì§‘ ë£¨í”„ì— `time.sleep()`ì„ ì¶”ê°€í•˜ì—¬ ê° ìš”ì²­ ì‚¬ì´ì—
  ì˜ë„ì ì¸ ì§€ì—° ì‹œê°„ì„ ë¶€ì—¬. `run_collect` í•¨ìˆ˜ì— `delay_seconds` íŒŒë¼ë¯¸í„°ë¥¼
  ì¶”ê°€í•˜ì—¬ ì§€ì—° ì‹œê°„ì„ ìœ ì—°í•˜ê²Œ ì¡°ì ˆí•  ìˆ˜ ìˆë„ë¡ ê°œì„ .
- íš¨ê³¼: ì•ˆì •ì ì¸ ë°ì´í„° ìˆ˜ì§‘ í™˜ê²½ì„ êµ¬ì¶•í•˜ê³  API ì„œë²„ ë¶€í•˜ë¥¼ ìµœì†Œí™”.
"""
import os
import gc
import shutil
import time  # ğŸ”´ API í˜¸ì¶œ ì§€ì—°ì„ ìœ„í•´ time ëª¨ë“ˆ ì„í¬íŠ¸
import pandas as pd
from typing import List, Tuple
from tqdm import tqdm
from pykrx import stock

# í”„ë¡œì íŠ¸ ìœ í‹¸ë¦¬í‹° ëª¨ë“ˆ ì„í¬íŠ¸
from modules.utils_io import (
    ensure_dir, read_yaml, to_date, yyyymmdd, retry_request,
    save_parquet_partitioned_monthly, write_meta, read_meta,
    load_partition_day,
    downcast_numeric
)
from modules.utils_logger import logger

REQUIRED_RAW_COLS = [
    "date", "code", "open", "high", "low", "close", "volume", "value",
    "inst_net_val", "foreign_net_val"
]

# [ì¶”ê°€] ì›”ë³„ ë°ì´í„°ì˜ ì™„ê²°ì„±ì„ ê²€ì‚¬í•˜ëŠ” í—¬í¼ í•¨ìˆ˜
def _check_month_completeness(ym_period: pd.Period, base_dir: str, all_trading_days: pd.DatetimeIndex) -> bool:
    """
    ì£¼ì–´ì§„ ì›”(YYYY-MM)ì˜ ë°ì´í„°ê°€ ë§ˆì§€ë§‰ ì˜ì—…ì¼ê¹Œì§€ ìˆ˜ì§‘ë˜ì—ˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    """
    path = os.path.join(base_dir, f"Y={ym_period.year}", f"M={ym_period.month:02d}", "part.parquet")
    
    if not os.path.exists(path):
        return False  # íŒŒì¼ì´ ì—†ìœ¼ë©´ ë¯¸ì™„ê²°

    try:
        # í•´ë‹¹ ì›”ì˜ ë§ˆì§€ë§‰ ì˜ì—…ì¼ ì°¾ê¸°
        last_trading_day_of_month = all_trading_days[
            (all_trading_days.year == ym_period.year) & (all_trading_days.month == ym_period.month)
        ].max()

        if pd.isna(last_trading_day_of_month):
            return True # í•´ë‹¹ ì›”ì— ì˜ì—…ì¼ì´ ì—†ìœ¼ë©´ ì™„ë£Œëœ ê²ƒìœ¼ë¡œ ê°„ì£¼

        # ì €ì¥ëœ ë°ì´í„°ì˜ ë§ˆì§€ë§‰ ë‚ ì§œ í™•ì¸
        df = pd.read_parquet(path, columns=['date'])
        last_date_in_file = pd.to_datetime(df['date']).max()

        # ë°ì´í„°ì˜ ë§ˆì§€ë§‰ ë‚ ì§œê°€ í•´ë‹¹ ì›”ì˜ ë§ˆì§€ë§‰ ì˜ì—…ì¼ê³¼ ê°™ê±°ë‚˜ í¬ë©´ ì™„ê²°
        return last_date_in_file >= last_trading_day_of_month

    except Exception as e:
        logger.warning(f"{ym_period} ì™„ê²°ì„± ê²€ì‚¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. ë¯¸ì™„ê²°ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
        return False

def date_blocks(start: pd.Timestamp, end: pd.Timestamp, months: int = 3) -> List[Tuple[pd.Timestamp, pd.Timestamp]]:
    """ì£¼ì–´ì§„ ì‹œì‘ì¼ê³¼ ì¢…ë£Œì¼ì„ nê°œì›” ë‹¨ìœ„ì˜ ë¸”ë¡ìœ¼ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤."""
    s = pd.Timestamp(start).normalize().replace(day=1)
    e = pd.Timestamp(end).normalize()
    out = []
    cur = s
    while cur <= e:
        nxt = (cur + pd.DateOffset(months=months)) - pd.Timedelta(days=1)
        if nxt > e:
            nxt = e
        out.append((cur, nxt))
        cur = nxt + pd.Timedelta(days=1)
    return out

def _rename_ohlcv_cols(df: pd.DataFrame) -> pd.DataFrame:
    """pykrx OHLCV ë°ì´í„°í”„ë ˆì„ì˜ ì»¬ëŸ¼ëª…ì„ í‘œì¤€ ì˜ë¬¸ëª…ìœ¼ë¡œ ë³€ê²½í•©ë‹ˆë‹¤."""
    rename_map = {
        "ë‚ ì§œ": "date", "Date": "date",
        "ì‹œê°€": "open", "ê³ ê°€": "high", "ì €ê°€": "low", "ì¢…ê°€": "close",
        "ê±°ë˜ëŸ‰": "volume", "ê±°ë˜ëŒ€ê¸ˆ": "value",
        "Open": "open", "High": "high", "Low": "low", "Close": "close",
        "Volume": "volume", "Value": "value"
    }
    return df.rename(columns={k: v for k, v in rename_map.items() if k in df.columns})

def _fetch_and_merge_daily_data(d: pd.Timestamp, required_cols: list) -> pd.DataFrame:
    """[v2.8.11 ì‹ ê·œ] íŠ¹ì •ì¼ì˜ ëª¨ë“  ê°€ê²©ê³¼ ìˆ˜ê¸‰ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ ë³‘í•©í•˜ëŠ” í†µí•© í•¨ìˆ˜."""
    day_str = yyyymmdd(d)

    # 1. ê°€ê²© ë°ì´í„°(OHLCV) ìˆ˜ì§‘
    try:
        df_kospi_px = retry_request(stock.get_market_ohlcv, date=day_str, market="KOSPI")
        df_kosdaq_px = retry_request(stock.get_market_ohlcv, date=day_str, market="KOSDAQ")
        df_px = pd.concat([df_kospi_px, df_kosdaq_px])
        
        if df_px.empty:
            return pd.DataFrame() 

        df_px.index.name = "code"
        df_px = df_px.reset_index()
    except Exception as e:
        logger.warning(f"{day_str} ê°€ê²© ë°ì´í„° ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return pd.DataFrame()

    df_px = _rename_ohlcv_cols(df_px)

    # 2. ìˆ˜ê¸‰ ë°ì´í„° ìˆ˜ì§‘ (íš¨ìœ¨ì ì¸ ìµœì¢… ë¡œì§)
    try:
        all_fund_flow_data = []
        for market in ["KOSPI", "KOSDAQ"]:
            df_foreign = retry_request(
                stock.get_market_net_purchases_of_equities, day_str, day_str, market, "ì™¸êµ­ì¸"
            )
            df_foreign = df_foreign[['ìˆœë§¤ìˆ˜ê±°ë˜ëŒ€ê¸ˆ']].rename(columns={'ìˆœë§¤ìˆ˜ê±°ë˜ëŒ€ê¸ˆ': 'foreign_net_val'})

            df_inst = retry_request(
                stock.get_market_net_purchases_of_equities, day_str, day_str, market, "ê¸°ê´€í•©ê³„"
            )
            df_inst = df_inst[['ìˆœë§¤ìˆ˜ê±°ë˜ëŒ€ê¸ˆ']].rename(columns={'ìˆœë§¤ìˆ˜ê±°ë˜ëŒ€ê¸ˆ': 'inst_net_val'})
            
            df_market_ff = pd.merge(df_foreign, df_inst, left_index=True, right_index=True, how='outer')
            all_fund_flow_data.append(df_market_ff)

        df_ff = pd.concat(all_fund_flow_data)
        df_ff.index.name = 'code'
        df_ff = df_ff.reset_index()

    except Exception as e:
        logger.warning(f"{day_str} ìˆ˜ê¸‰ ë°ì´í„° ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. ê°€ê²© ë°ì´í„°ë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
        df_ff = pd.DataFrame()

    # 3. ë°ì´í„° ì²˜ë¦¬ ë° ë³‘í•©
    if not df_ff.empty:
        df_px['code'] = df_px['code'].astype(str)
        df_ff['code'] = df_ff['code'].astype(str)
        df_merged = pd.merge(df_px, df_ff, on="code", how="left")
    else:
        df_merged = df_px.copy()

    for col in ['inst_net_val', 'foreign_net_val']:
        if col not in df_merged.columns:
            df_merged[col] = 0.0
    df_merged[['inst_net_val', 'foreign_net_val']] = df_merged[['inst_net_val', 'foreign_net_val']].fillna(0)

    # 4. ìµœì¢… ë°ì´í„° ì •ì œ
    df_merged["date"] = pd.to_datetime(d.date())
    df_merged["code"] = df_merged["code"].astype(str)
    
    final_cols = [col for col in required_cols if col in df_merged.columns]
    df_final = df_merged[final_cols]
    
    df_final = df_final.dropna(subset=["date", "code", "open", "close", "value"])
    df_final = df_final[(df_final["open"] > 0) & (df_final["close"] > 0) & (df_final["value"] >= 0)]
    
    return downcast_numeric(df_final, 
                            price_cols=["open", "high", "low", "close"], 
                            value_cols=["value", "inst_net_val", "foreign_net_val"], 
                            vol_cols=["volume"])
    # [ìˆ˜ì •] í•˜ë“œì½”ë”©ëœ ë¦¬ìŠ¤íŠ¸ ëŒ€ì‹ , ëª¨ë“ˆ ìƒë‹¨ì— ì •ì˜ëœ ìƒìˆ˜ë¥¼ ì‚¬ìš©
    #final_cols = [col for col in REQUIRED_RAW_COLS if col in df_merged.columns]
    #df_final = df_merged[final_cols]
    
    #df_final = df_final.dropna(subset=["date", "code", "open", "close", "value"])
    
# [ì¶”ê°€] KOSPI ì§€ìˆ˜ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  ì €ì¥í•˜ëŠ” ë‚´ë¶€ í•¨ìˆ˜
def _collect_and_save_index_data(start_d: pd.Timestamp, end_d: pd.Timestamp, index_path: str):
    """KOSPI ì§€ìˆ˜ ë°ì´í„°ë¥¼ ì¦ë¶„ ë°©ì‹ìœ¼ë¡œ ìˆ˜ì§‘í•˜ì—¬ ë‹¨ì¼ Parquet íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    ensure_dir(index_path)
    index_file = os.path.join(index_path, "kospi.parquet")
    
    # ê¸°ì¡´ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ë§ˆì§€ë§‰ ë‚ ì§œë¥¼ í™•ì¸í•˜ì—¬ ì´í›„ ë°ì´í„°ë§Œ ìš”ì²­
    if os.path.exists(index_file):
        existing_df = pd.read_parquet(index_file)
        last_date = existing_df['date'].max()
        fetch_start_d = last_date + pd.Timedelta(days=1)
    else:
        existing_df = pd.DataFrame()
        fetch_start_d = start_d

    if fetch_start_d > end_d:
        logger.info("KOSPI ì§€ìˆ˜ ë°ì´í„°ëŠ” ì´ë¯¸ ìµœì‹  ìƒíƒœì…ë‹ˆë‹¤.")
        return

    logger.info(f"KOSPI ì§€ìˆ˜ ë°ì´í„° ìˆ˜ì§‘: {fetch_start_d.date()} ~ {end_d.date()}")
    try:
        # ë„‰ë„‰í•˜ê²Œ ì´ì „ ë°ì´í„°ë¥¼ í¬í•¨í•˜ì—¬ ìš”ì²­ í›„ í•„í„°ë§ (MA ê³„ì‚° ë“±ì„ ìœ„í•´)
        s_date_str = (fetch_start_d - pd.DateOffset(days=365)).strftime('%Y%m%d')
        e_date_str = end_d.strftime('%Y%m%d')
        
        new_df = retry_request(stock.get_index_ohlcv, s_date_str, e_date_str, "1001").reset_index()
        new_df = _rename_ohlcv_cols(new_df)
        new_df['date'] = pd.to_datetime(new_df['date'])

        # ê¸°ì¡´ ë°ì´í„°ì™€ ë³‘í•© í›„ ì¤‘ë³µ ì œê±°
        combined_df = pd.concat([existing_df, new_df]).drop_duplicates(subset=['date'], keep='last').sort_values('date')
        combined_df.to_parquet(index_file, index=False, compression="zstd")
        logger.info(f"KOSPI ì§€ìˆ˜ ë°ì´í„°ê°€ '{index_file}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    except Exception as e:
        logger.error(f"KOSPI ì§€ìˆ˜ ë°ì´í„° ìˆ˜ì§‘/ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)

# [ìˆ˜ì •] run_collect í•¨ìˆ˜ë¥¼ ì•„ë˜ ì½”ë“œë¡œ ì „ì²´ êµì²´í•©ë‹ˆë‹¤.
# modules/collect.py
def run_collect(settings_path: str, start: str = None, end: str = None, use_meta: bool = True, overwrite: bool = False, delay_seconds: float = 0.5):
    """
    ë°ì´í„° ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸ì˜ ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜.
    """
    cfg = read_yaml(settings_path)
    paths = cfg["paths"]

    if overwrite:
        for key in ["raw_prices", "raw_fundflow", "merged", "raw_index"]:
            if paths.get(key) and os.path.exists(paths[key]):
                logger.warning(f"Overwrite: ê¸°ì¡´ '{paths[key]}' ë””ë ‰í„°ë¦¬ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤.")
                shutil.rmtree(paths[key])
        use_meta = False

    for p in paths.values(): ensure_dir(p)

    today = pd.Timestamp.today().normalize()
    start_d = to_date(start) if start else to_date('2020-01-01')
    end_d = to_date(end) if end else today

    if use_meta and not overwrite:
        last = read_meta(paths["meta"], "last_collected_date")
        if last: start_d = max(start_d, to_date(last) + pd.Timedelta(days=1))

    if start_d > end_d:
        logger.info("ìˆ˜ì§‘í•  ì‹ ê·œ ë°ì´í„° êµ¬ê°„ì´ ì—†ìŠµë‹ˆë‹¤.")
        return True

    logger.info(f"ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘: {start_d.date()} ~ {end_d.date()}")
    
    try:
        logger.info("ì „ì²´ ìˆ˜ì§‘ ê¸°ê°„ì˜ ì‹¤ì œ ì˜ì—…ì¼ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤...")
        all_trading_days_full_period = stock.get_index_ohlcv('20200101', end_d.strftime('%Y%m%d'), "1001").index
    except Exception as e:
        logger.error(f"ì˜ì—…ì¼ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. ë°ì´í„° ìˆ˜ì§‘ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        return False

    trading_days_in_period = all_trading_days_full_period[
        (all_trading_days_full_period >= start_d) & (all_trading_days_full_period <= end_d)
    ]
    
    # [ìˆ˜ì •] ì´ ë¼ì¸ì„ ì¶”ê°€í•˜ì—¬ 'overwrite=True'ì¼ ë•Œ final_days_to_collect ë³€ìˆ˜ê°€ ì •ì˜ë˜ë„ë¡ í•©ë‹ˆë‹¤.
    final_days_to_collect = trading_days_in_period

    if not overwrite:
        logger.info("="*50)
        logger.info("      <<< ì›”ë³„ ë°ì´í„° ì™„ê²°ì„± ê²€ì‚¬ ì‹œì‘ >>>")
        logger.info("="*50)
        
        months_to_check = sorted(list(set(pd.PeriodIndex(trading_days_in_period, freq='M'))))
        
        incomplete_months = []
        for ym_period in tqdm(months_to_check, desc="Checking Month Completeness"):
            is_last_month = (ym_period == months_to_check[-1]) if months_to_check else False
            if is_last_month or not _check_month_completeness(ym_period, paths["merged"], all_trading_days_full_period):
                incomplete_months.append(ym_period)
            else:
                logger.info(f"â˜‘ï¸  {ym_period} ë°ì´í„°ëŠ” ì´ë¯¸ ì™„ê²°ë˜ì–´ ê±´ë„ˆëœë‹ˆë‹¤.")
        
        if not incomplete_months:
            logger.info("ëª¨ë“  ì›”ë³„ ë°ì´í„°ê°€ ì™„ê²°ë˜ì—ˆìŠµë‹ˆë‹¤. ìˆ˜ì§‘í•  ì‹ ê·œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            write_meta(paths["meta"], "last_collected_date", str(end_d.date()))
            return True

        final_days_to_collect = trading_days_in_period[
            pd.to_datetime(trading_days_in_period).to_period('M').isin(incomplete_months)
        ]

    if final_days_to_collect.empty:
        logger.info("ìˆ˜ì§‘í•  ì˜ì—…ì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return True

    logger.info(f"ì´ {len(final_days_to_collect)}ì¼ì˜ ì˜ì—…ì¼ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")
    
    all_data_collected = []
    for day in tqdm(final_days_to_collect, desc="ì¼ë³„ ë°ì´í„° í†µí•© ìˆ˜ì§‘"):
        daily_merged_data = _fetch_and_merge_daily_data(day, REQUIRED_RAW_COLS)
        if daily_merged_data is not None and not daily_merged_data.empty:
            all_data_collected.append(daily_merged_data)
        time.sleep(delay_seconds)
    
    if all_data_collected:
        block_df = pd.concat(all_data_collected, ignore_index=True)
        save_parquet_partitioned_monthly(block_df, paths["merged"])
        del block_df, all_data_collected
        gc.collect()
    else:
        logger.warning("ìˆ˜ì§‘ëœ ì‹ ê·œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    logger.info("="*50)
    logger.info("      <<< KOSPI ì§€ìˆ˜ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ >>>")
    logger.info("="*50)
    _collect_and_save_index_data(to_date('2020-01-01'), end_d, paths["raw_index"])

    try:
        df_check = load_partition_day(paths["merged"], start_d, end_d)

        if df_check.empty:
            if not trading_days_in_period.empty:
                logger.error(f"ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: ì˜ì—…ì¼({len(trading_days_in_period)}ì¼) ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return False
            else:
                logger.info("ìˆ˜ì§‘ ê¸°ê°„ì— ì˜ì—…ì¼ì´ ì—†ì–´ ì‹ ê·œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. (ì •ìƒ)")
                write_meta(paths["meta"], "last_collected_date", str(end_d.date()))
                return True

        last_saved = df_check['date'].max()
        if pd.notna(last_saved):
             write_meta(paths["meta"], "last_collected_date", str(end_d.date()))
             logger.info(f"ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ. last_collected_date = {end_d.date()}")
             return True
        else:
            logger.error(f"ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: ìµœì‹  ë°ì´í„° ëˆ„ë½ (ë§ˆì§€ë§‰ ì €ì¥ì¼: {last_saved})")
            return False
    except Exception as e:
        logger.error(f"ìˆ˜ì§‘ ì„±ê³µ ì—¬ë¶€ í™•ì¸ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}", exc_info=True)
        return False