# modules/utils_db.py (ver 2.7.10)
"""DB ê´€ë¦¬ ìœ í‹¸ë¦¬í‹° ëª¨ë“ˆ.

[v2.7.10] SHAP ë¶„ì„ ê²°ê³¼ ë¡œê¹… ê¸°ëŠ¥ ì¶”ê°€
- SHAP ë¶„ì„ ê²°ê³¼ë¥¼ ì €ì¥í•˜ê¸° ìœ„í•œ 'shap_importance_log' í…Œì´ë¸” ì •ì˜ ì¶”ê°€.
- SHAP ë¶„ì„ ê²°ê³¼ë¥¼ DBì— ì‚½ì…í•˜ëŠ” 'insert_shap_results' í•¨ìˆ˜ ì‹ ì„¤.
"""
import sqlite3
import pandas as pd
import json
import numpy as np
from datetime import datetime, timedelta
import hashlib
import time
import os
import typing

if typing.TYPE_CHECKING:
    import optuna

from .utils_logger import logger

DB_PATH = "data/db/spikehunter_log.db"
TABLE_DEFINITIONS = {
    "backtest_summary": """
        CREATE TABLE IF NOT EXISTS backtest_summary (
            backtest_id TEXT PRIMARY KEY,
            run_timestamp TEXT NOT NULL,
            strategy_name TEXT,
            start_date TEXT,
            end_date TEXT,
            cagr REAL,
            sharpe REAL,
            mdd REAL,
            total_trades INTEGER,
            win_rate REAL,
            params_json TEXT,
            equity_file_path TEXT
        );
    """,
    "trade_log": """
        CREATE TABLE IF NOT EXISTS trade_log (
            trade_id INTEGER PRIMARY KEY AUTOINCREMENT,
            backtest_id TEXT,
            entry_date TEXT,
            exit_date TEXT,
            code TEXT,
            return REAL,
            reason TEXT,
            FOREIGN KEY (backtest_id) REFERENCES backtest_summary (backtest_id)
        );
    """,
    "optimization_log": """
        CREATE TABLE IF NOT EXISTS optimization_log (
            log_id INTEGER PRIMARY KEY AUTOINCREMENT,
            study_name TEXT NOT NULL,
            trial_number INTEGER NOT NULL,
            state TEXT,
            value REAL,
            params_json TEXT,
            run_timestamp TEXT
        );
    """,
    # ğŸ”´ [ì¶”ê°€] SHAP ë¶„ì„ ê²°ê³¼ ì €ì¥ì„ ìœ„í•œ ì‹ ê·œ í…Œì´ë¸”
    "shap_importance_log": """
        CREATE TABLE IF NOT EXISTS shap_importance_log (
            log_id INTEGER PRIMARY KEY AUTOINCREMENT,
            run_id TEXT NOT NULL,
            analysis_type TEXT NOT NULL,
            item_name TEXT NOT NULL,
            mean_abs_shap REAL,
            rank INTEGER,
            run_timestamp TEXT
        );
    """,
    # ğŸ”´ [ì¶”ê°€] ì‹¤ì „/ëª¨ì˜ ê²€ì¦ìš© ì¶”ì²œ ì¢…ëª© ì¶”ì  í…Œì´ë¸”
    "daily_signals": """
        CREATE TABLE IF NOT EXISTS daily_signals (
            signal_id INTEGER PRIMARY KEY AUTOINCREMENT,
            date TEXT NOT NULL,
            code TEXT NOT NULL,
            name TEXT,
            strategy_name TEXT,
            ml_score REAL,
            rank INTEGER,
            
            -- ì§„ì… ì¡°ê±´ (ê¸°ë¡ìš©)
            entry_price REAL, -- ì¶”ì²œì¼ ì¢…ê°€ (ê¸°ì¤€ê°€)
            target_price REAL,
            stop_price REAL,
            target_rate REAL, -- [ì¶”ê°€] ëª©í‘œ ìˆ˜ìµë¥  (0.10 ë“±)
            stop_rate REAL,   -- [ì¶”ê°€] ì†ì ˆë¥  (-0.05 ë“±)
            max_hold_days INTEGER,
            
            -- ìƒíƒœ ì¶”ì 
            status TEXT DEFAULT 'PENDING', -- PENDING, WIN, LOSS, TIME_OUT
            exit_date TEXT,
            exit_price REAL,
            return_rate REAL,
            
            -- ëª¨ë‹ˆí„°ë§
            highest_price REAL, -- ê¸°ê°„ ë‚´ ìµœê³ ê°€
            lowest_price REAL,  -- ê¸°ê°„ ë‚´ ìµœì €ê°€
            holding_days INTEGER DEFAULT 0,
            
            created_at TEXT
        );
    """
}

# [ìˆ˜ì •] _sanitize_for_db í•¨ìˆ˜ë¥¼ ë” ì•ˆì •ì ì¸ ë²„ì „ìœ¼ë¡œ êµì²´í•©ë‹ˆë‹¤.
def _sanitize_for_db(value):
    """NumPy/Pandas íƒ€ì…ì„ DBì— ì €ì¥ ê°€ëŠ¥í•œ íŒŒì´ì¬ ê¸°ë³¸ íƒ€ì…ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    # pd.isnaëŠ” None, np.nan ë“±ì„ ëª¨ë‘ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    if pd.isna(value):
        return None
    # numpy float ë˜ëŠ” python floatë¥¼ python floatìœ¼ë¡œ ëª…ì‹œì  ë³€í™˜
    if isinstance(value, (np.floating, float)):
        return float(value)
    # numpy int ë˜ëŠ” python intë¥¼ python intë¡œ ëª…ì‹œì  ë³€í™˜
    if isinstance(value, (np.integer, int)):
        return int(value)
    if isinstance(value, np.ndarray):
        return value.tolist()
    if isinstance(value, pd.Timestamp):
        return value.isoformat()
    return value # ê·¸ ì™¸ íƒ€ì…(ì£¼ë¡œ str)ì€ ê·¸ëŒ€ë¡œ ë°˜í™˜

def get_db_connection():
    """DB ì—°ê²°ì„ ìƒì„±í•˜ê³  ë°˜í™˜í•©ë‹ˆë‹¤."""
    os.makedirs(os.path.dirname(DB_PATH), exist_ok=True)
    return sqlite3.connect(DB_PATH, timeout=30)

def _migrate_schema(conn):
    """ê¸°ì¡´ í…Œì´ë¸”ì˜ ìŠ¤í‚¤ë§ˆë¥¼ ìµœì‹  ë³€ê²½ì‚¬í•­ì— ë§ì¶° ë§ˆì´ê·¸ë ˆì´ì…˜í•©ë‹ˆë‹¤."""
    try:
        cursor = conn.cursor()
        
        # 1. daily_signals í…Œì´ë¸” ì»¬ëŸ¼ ì¶”ê°€ (target_rate, stop_rate)
        # 2026-01-08: ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ í›„ ì¶”ê°€
        cursor.execute("PRAGMA table_info(daily_signals)")
        columns = [info[1] for info in cursor.fetchall()]
        
        if 'daily_signals' in columns or columns: # í…Œì´ë¸”ì´ ì¡´ì¬í•˜ëŠ” ê²½ìš°ì—ë§Œ
            if 'target_rate' not in columns:
                logger.info("ë§ˆì´ê·¸ë ˆì´ì…˜: daily_signalsì— target_rate ì»¬ëŸ¼ ì¶”ê°€")
                cursor.execute("ALTER TABLE daily_signals ADD COLUMN target_rate REAL")
            
            if 'stop_rate' not in columns:
                logger.info("ë§ˆì´ê·¸ë ˆì´ì…˜: daily_signalsì— stop_rate ì»¬ëŸ¼ ì¶”ê°€")
                cursor.execute("ALTER TABLE daily_signals ADD COLUMN stop_rate REAL")
                
    except Exception as e:
        logger.error(f"ìŠ¤í‚¤ë§ˆ ë§ˆì´ê·¸ë ˆì´ì…˜ ì¤‘ ì˜¤ë¥˜: {e}")

def create_tables():
    """í”„ë¡œê·¸ë¨ ì‹œì‘ ì‹œ í•„ìš”í•œ ëª¨ë“  í…Œì´ë¸”ì„ ìƒì„±í•˜ê±°ë‚˜ ê²€ì¦í•©ë‹ˆë‹¤."""
    try:
        with get_db_connection() as conn:
            cursor = conn.cursor()
            for table_name, ddl_script in TABLE_DEFINITIONS.items():
                cursor.execute(ddl_script)
            
            # [ì¶”ê°€] ìŠ¤í‚¤ë§ˆ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰
            _migrate_schema(conn)
            
            conn.commit()
        logger.info(f"ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸”ì´ ì„±ê³µì ìœ¼ë¡œ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤. (ê²½ë¡œ: {DB_PATH})")
    except Exception as e:
        logger.error(f"DB í…Œì´ë¸” ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)

def insert_backtest_results(backtest_id: str, metrics: dict, params: dict, tradelog_df: pd.DataFrame, equity_path: str, start_date: str, end_date: str, strategy_name: str):
    """ë°±í…ŒìŠ¤íŠ¸ ìµœì¢… ê²°ê³¼ë¥¼ DBì— ì‚½ì…í•©ë‹ˆë‹¤."""
    # ... ê¸°ì¡´ ì½”ë“œ ...
    params_str = json.dumps({k: _sanitize_for_db(v) for k, v in params.items()}, ensure_ascii=False)

    if len(params_str) > 5000:
        logger.warning(f"DBì— ì €ì¥ë  íŒŒë¼ë¯¸í„° JSONì˜ ê¸¸ì´ê°€ 5000ìë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤ (ê¸¸ì´: {len(params_str)}).")

    summary_data = {
        'backtest_id': backtest_id,
        'run_timestamp': datetime.now().isoformat(),
        'strategy_name': strategy_name,
        'start_date': start_date,
        'end_date': end_date,
        'cagr': _sanitize_for_db(metrics.get('CAGR_raw', 0.0)),
        'sharpe': _sanitize_for_db(metrics.get('Sharpe_raw', 0.0)),
        'mdd': _sanitize_for_db(metrics.get('MDD_raw', 0.0)),
        'total_trades': _sanitize_for_db(metrics.get('ì´ê±°ë˜íšŸìˆ˜', 0)),
        'win_rate': _sanitize_for_db(metrics.get('win_rate_raw', 0.0)),
        'params_json': params_str,
        'equity_file_path': equity_path
    }

    try:
        with get_db_connection() as conn:
            summary_df = pd.DataFrame([summary_data])
            summary_df.to_sql('backtest_summary', conn, if_exists='append', index=False)

            if not tradelog_df.empty:
                tradelog_to_insert = tradelog_df.copy()
                tradelog_to_insert['backtest_id'] = backtest_id

                tradelog_to_insert['entry_date'] = pd.to_datetime(tradelog_to_insert['entry_date']).map(lambda x: x.isoformat())
                tradelog_to_insert['exit_date'] = pd.to_datetime(tradelog_to_insert['exit_date']).map(lambda x: x.isoformat())

                tradelog_to_insert = tradelog_to_insert[['backtest_id', 'entry_date', 'exit_date', 'code', 'return', 'reason']]
                tradelog_to_insert.to_sql('trade_log', conn, if_exists='append', index=False)

            logger.info(f"ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ê°€ DBì— ì„±ê³µì ìœ¼ë¡œ ê¸°ë¡ë˜ì—ˆìŠµë‹ˆë‹¤ (ID: {backtest_id}).")
    except Exception as e:
        logger.error(f"DB ê¸°ë¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)

def insert_optimization_logs(study: 'optuna.study.Study'):
    """ì™„ë£Œëœ Optuna ìŠ¤í„°ë””ì˜ ëª¨ë“  Trial ê²°ê³¼ë¥¼ DBì— ì¼ê´„ ì‚½ì…í•©ë‹ˆë‹¤."""
    # ... ê¸°ì¡´ ì½”ë“œ ...
    if not study:
        return

    records = []
    run_ts = datetime.now().isoformat()
    for trial in study.trials:
        records.append({
            'study_name': study.study_name,
            'trial_number': trial.number,
            'state': trial.state.name,
            'value': trial.value,
            'params_json': json.dumps(trial.params),
            'run_timestamp': run_ts
        })

    if not records:
        logger.warning(f"'{study.study_name}' ìŠ¤í„°ë””ì—ì„œ DBì— ê¸°ë¡í•  Trialì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    try:
        with get_db_connection() as conn:
            df = pd.DataFrame(records)
            df.to_sql('optimization_log', conn, if_exists='append', index=False)
            logger.info(f"'{study.study_name}' ìŠ¤í„°ë””ì˜ Trial {len(records)}ê°œê°€ DBì— ì„±ê³µì ìœ¼ë¡œ ê¸°ë¡ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        logger.error(f"ìµœì í™” ë¡œê·¸ DB ê¸°ë¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)

# ğŸ”´ [ì¶”ê°€] SHAP ë¶„ì„ ê²°ê³¼ë¥¼ DBì— ì €ì¥í•˜ëŠ” í•¨ìˆ˜
def insert_shap_results(run_id: str, analysis_type: str, shap_df: pd.DataFrame):
    """
    SHAP ë¶„ì„ ê²°ê³¼(í”¼ì²˜ ì¤‘ìš”ë„ ë“±)ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ê¸°ë¡í•©ë‹ˆë‹¤.
    """
    if shap_df.empty:
        return
        
    records = shap_df.copy()
    records['run_id'] = run_id
    records['analysis_type'] = analysis_type
    records['run_timestamp'] = datetime.now().isoformat()
    
    # DB í…Œì´ë¸” ì»¬ëŸ¼ ìˆœì„œì— ë§ê²Œ ì¬ì •ë ¬
    records = records[['run_id', 'analysis_type', 'item_name', 'mean_abs_shap', 'rank', 'run_timestamp']]
    
    try:
        with get_db_connection() as conn:
            records.to_sql('shap_importance_log', conn, if_exists='append', index=False)
        logger.info(f"SHAP ë¶„ì„ ê²°ê³¼({analysis_type}, {len(records)}ê°œ)ê°€ DBì— ì„±ê³µì ìœ¼ë¡œ ê¸°ë¡ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        logger.error(f"SHAP ë¶„ì„ ê²°ê³¼ DB ê¸°ë¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)

# ğŸ”´ [ì¶”ê°€] ì¶”ì²œ ì¢…ëª©(Signal) ê´€ë¦¬ í•¨ìˆ˜ë“¤

def insert_daily_signals(signals: pd.DataFrame, strategy_name: str, target_rate: float = 0.10, stop_rate: float = -0.05):
    """ì˜¤ëŠ˜ì˜ ì¶”ì²œ ì¢…ëª©ë“¤ì„ DBì— ë“±ë¡í•©ë‹ˆë‹¤. (í•´ë‹¹ ì¼ìì˜ ê¸°ì¡´ ê¸°ë¡ì€ ì œê±° - ìµœì‹ ë³¸ ìœ ì§€)"""
    if signals.empty: return
    
    current_time = datetime.now().isoformat()
    signals_to_insert = []
    
    # [ìˆ˜ì •] ë‚ ì§œë³„ë¡œ ê¸°ì¡´ ë°ì´í„°ë¥¼ ì‚­ì œí•˜ê¸° ìœ„í•´ ì²˜ë¦¬ ëŒ€ìƒ ë‚ ì§œë¥¼ ìˆ˜ì§‘
    target_dates = set()

    for _, row in signals.iterrows():
        # ê¸°ë³¸ ì •ë³´
        close_price = float(row['close'])
        
        # [ìˆ˜ì •] ì™¸ë¶€ì—ì„œ ì£¼ì…ë°›ì€ ì „ëµ íŒŒë¼ë¯¸í„° ì‚¬ìš©
        entry_price = close_price
        target_price = entry_price * (1 + target_rate)
        stop_price = entry_price * (1 + stop_rate)
        max_hold = 5
        
        date_str = row['date'].strftime('%Y-%m-%d') if isinstance(row['date'], pd.Timestamp) else str(row['date'])[:10]
        target_dates.add(date_str)

        record = {
            'date': date_str,
            'code': str(row['code']),
            'name': row.get('name', ''),
            'strategy_name': strategy_name,
            'ml_score': float(row.get('ml_score', 0.0)),
            'rank': int(row.get('rank', 0)),
            'entry_price': entry_price,
            'target_price': target_price,
            'stop_price': stop_price,
            'target_rate': target_rate,
            'stop_rate': stop_rate,
            'max_hold_days': max_hold,
            'status': 'PENDING',
            'highest_price': entry_price,
            'lowest_price': entry_price,
            'created_at': current_time
        }
        signals_to_insert.append(record)
        
    try:
        with get_db_connection() as conn:
            cursor = conn.cursor()
            
            # [ìˆ˜ì •] í•´ë‹¹ ë‚ ì§œì˜ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ (Overwrite)
            for d in target_dates:
                # logger.info(f"ê¸°ì¡´ ì¶”ì²œ ì‹ í˜¸ ì‚­ì œ(Overwrite) - Date: {d}")
                cursor.execute("DELETE FROM daily_signals WHERE date = ?", (d,))
            
            # ì‹ ê·œ ë°ì´í„° ì‚½ì…
            for rec in signals_to_insert:
                cols = ', '.join(rec.keys())
                placeholders = ', '.join(['?'] * len(rec))
                sql = f"INSERT INTO daily_signals ({cols}) VALUES ({placeholders})"
                cursor.execute(sql, list(rec.values()))
            
            conn.commit()
            logger.info(f"{len(signals_to_insert)}ê°œì˜ ì‹ ê·œ ì¶”ì²œ ì‹ í˜¸ë¥¼ DBì— ë“±ë¡í–ˆìŠµë‹ˆë‹¤. (ê¸°ì¡´ {len(target_dates)}ì¼ì¹˜ ë°ì´í„° ë®ì–´ì”€)")
            
    except Exception as e:
        logger.error(f"ì¶”ì²œ ì‹ í˜¸ ë“±ë¡ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)

def get_pending_signals() -> pd.DataFrame:
    """ê²€ì¦ì´ ì™„ë£Œë˜ì§€ ì•Šì€(PENDING) ì‹ í˜¸ë“¤ì„ ì¡°íšŒí•©ë‹ˆë‹¤."""
    try:
        with get_db_connection() as conn:
            df = pd.read_sql("SELECT * FROM daily_signals WHERE status = 'PENDING'", conn)
            return df
    except Exception as e:
        logger.error(f"Pending ì‹ í˜¸ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        return pd.DataFrame()

def get_recent_signals(limit_days: int = 30) -> pd.DataFrame:
    """ìµœê·¼ Nì¼ ì´ë‚´ì— ìƒì„±ëœ ì‹ í˜¸ë“¤ì„ ì¡°íšŒí•©ë‹ˆë‹¤ (ìƒíƒœ ë¬´ê´€)."""
    try:
        with get_db_connection() as conn:
            # sqliteì˜ date í•¨ìˆ˜ ì‚¬ìš©: date(created_at) >= date('now', '-30 days')
            # í•˜ì§€ë§Œ created_at í¬ë§·ì´ isoformat()ì´ë¼ TEXT ë¹„êµë„ ê°€ëŠ¥í•˜ì§€ë§Œ, 
            # date ì»¬ëŸ¼ì„ ê¸°ì¤€ìœ¼ë¡œ í•˜ëŠ” ê²ƒì´ ë” ì •í™•í•¨.
            cutoff_date = (datetime.now() - timedelta(days=limit_days)).strftime('%Y-%m-%d')
            sql = f"SELECT * FROM daily_signals WHERE date >= '{cutoff_date}' ORDER BY date DESC, rank ASC"
            df = pd.read_sql(sql, conn)
            return df
    except Exception as e:
        logger.error(f"ìµœê·¼ ì‹ í˜¸ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        return pd.DataFrame()

def update_signal_outcome(signal_id: int, status: str, exit_date: str, exit_price: float, 
                          return_rate: float, high: float, low: float, holding_days: int):
    """ì‹ í˜¸ì˜ ìµœì¢… ê²°ê³¼ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
    try:
        with get_db_connection() as conn:
            cursor = conn.cursor()
            sql = """
                UPDATE daily_signals 
                SET status = ?, exit_date = ?, exit_price = ?, return_rate = ?,
                    highest_price = ?, lowest_price = ?, holding_days = ?
                WHERE signal_id = ?
            """
            cursor.execute(sql, (status, exit_date, exit_price, return_rate, high, low, holding_days, signal_id))
            conn.commit()
    except Exception as e:
        logger.error(f"ì‹ í˜¸ ê²°ê³¼ ì—…ë°ì´íŠ¸ ì˜¤ë¥˜ (ID: {signal_id}): {e}")

def update_signal_intermediate(signal_id: int, high: float, low: float, holding_days: int):
    """ì‹ í˜¸ì˜ ì¤‘ê°„ ìƒíƒœ(ìµœê³ /ìµœì €ê°€ ë“±)ë§Œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
    try:
        with get_db_connection() as conn:
            cursor = conn.cursor()
            sql = """
                UPDATE daily_signals 
                SET highest_price = ?, lowest_price = ?, holding_days = ?
                WHERE signal_id = ?
            """
            cursor.execute(sql, (high, low, holding_days, signal_id))
            conn.commit()
    except Exception as e:
        logger.error(f"ì‹ í˜¸ ì¤‘ê°„ ìƒíƒœ ì—…ë°ì´íŠ¸ ì˜¤ë¥˜ (ID: {signal_id}): {e}")
